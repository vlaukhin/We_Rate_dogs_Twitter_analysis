{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "After Gathering required data sets i.e. data from supplied tweeter archive, downloaded image_predictions and scrapped tweeter API data, which was quite a standalone task itself I have approached my data wrangling with standard Gather-Assess-Clean- Analyze technique.\n",
    "The first step in assessment was to give a glance look at the data structures both visual and programmatic, which helped to identify some quality and tidiness issues. It should be said that gathered data had quite a lot of quality and tidiness issues.\n",
    "\n",
    "## Assessment/Cleaning\n",
    " \n",
    " ### Tidiness\n",
    "-\t Iâ€™ve got to reshape doggo, floofer, pupper, puppo columns to be represented in 1 for initial tweeter archive. **This was perfomed through melt,  lambda x and text extract functions **\n",
    "-\t Stack and reshape image predictions p1-3 values to be represented as long format. **This was fixed with lreshape, melt, concat and to_numetic functions**\n",
    "\n",
    "### Quality\n",
    "Since gathered data had a lot of quality issues I have picked a limited set that were essential in my opinion:\n",
    "\n",
    "#### ` twitter_archive`  and ` twitter_API` combined   \n",
    "- 2356 and 2347 record in two tables. **Fixed by rename first and then merge by tweet_id**\n",
    "-  in_reply_to_status_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp, expanded_urls **were deleted by drop**\n",
    "- rename id to tweet_id from twitter_API **fixed using rename**\n",
    "\n",
    "#### `twitter_archive`  \n",
    "- Lots of NaNs in in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp. Also, they have way less non-nulls as the whole df. **Changed types for all needed and dropped unneeded**\n",
    "- Lots of Nones in name, doggo, floofer, pupper, puppo, **Fixed by transferring to long data though still insufficient data in the column**\n",
    "- Timestamp and retweeted_status_timestamp should be datetime object. **Fixed by pd.to_datetime convert the field**\n",
    "- Name values need to be checked, the missing ones should be replaced to None, 'my' should be replaced by Zoey, 'his' should be replaced by Quizno, all other values to 'None'. The name extraction was quite good though there are still values such as 'a', 'actually', 'all', 'an', 'by', 'getting', 'incredibly', 'infuriating', 'just', 'life', 'light', 'mad', 'not', 'officially', 'old', 'one', 'quite', 'space', 'such', 'the', 'this', 'unacceptable', 'very', 'my and 'his' present in the df.** fixed manually replacing those values**\n",
    "\n",
    "\n",
    "- retweeted_status_id has to be int64, **fixed by type change**\n",
    "- Would need to create real_rating column which will calculate rating_numerator/rating_denomerator **created with pandas**\n",
    "- Drop all columns with insufficient data, **fixed using drop**\n",
    "- all other values to 'None' **fixed with rename**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ` image_predictions` table\n",
    "- dog value column should be lowercase. **Fixed by string to lowercase method**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Created data frames\n",
    "\n",
    "List of created data frames to assist data wrangling:\n",
    "\n",
    "- **twitter_archive_merged** - merged twitter_archive (from twitter_archive_enhanced.csv) and twitter_API(from twitter API)\n",
    "\n",
    "Following dfs were used in the final data analysis\n",
    "- **twitter_image_max** - merged image_predictions (from image_predictions.tsv) and twitter_archive_merged, with identification of max algorithm probaility per each tweet_id\n",
    "\n",
    "- **twitter_image_med** - merged image_predictions (from image_predictions.tsv) and twitter_archive_merged, with identification of median algorithm probaility per each tweet_id\n",
    "\n",
    "- **twitter_image_min** - merged image_predictions (from image_predictions.tsv) and twitter_archive_merged, with identification of min algorithm probaility per each tweet_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set is a big sandbox where data wrangling becomes real fun. Although i tried my best to identify all issues. I bet there is even more of those hiding somwhere in a shade. Hence we need to have more time and domain knowledge to perform accurate EDA on this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Created By Volodymyr Laukhin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
